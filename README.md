# Misinformation Detection in Large Language Models using Graph Structures on Prompting Generation

Welcome to the GitHub repository for the research project conducted at the University of Amsterdam, focusing on "Misinformation Detection in Large Language Models using Graph Structures on Prompting Generation." This project explores approaches to enhance the trustworthiness of large language models by leveraging graph structures for the detection and mitigation of misinformation.

## Overview

The utilization of Large Language Models (LLMs) has become increasingly widespread, reaching not only researchers and developers but also the general public. These models have found applications in various domains, from assisting with everyday queries to providing recommendations and information on a wide range of topics. With their ubiquity, there comes a natural tendency for people to trust the information LLMs provide, often taking their responses as truths. This makes it more critical than ever to develop robust mechanisms for identifying and countering misinformation generated by these models. LLMs, although incredibly powerful, are not error-free and may sometimes produce incorrect or misleading responses. Thus, ensuring the accuracy and trustworthiness of the information given by these models is a pivotal concern as they continue to shape the way we interact with information in our daily lives.

We explore the behavior of large language models like ChatGPT when prompted with a question and the subsequent responses they generate. Specifically, we investigate the model's ability to adapt and change its answer when informed that its initial response is incorrect. We observe the seemingly endless adaptability of the model. It can continue to generate different responses to the same question. One of the central questions we address is: How can we determine which of the multiple answers provided by the model is correct? 

## Directory structure

An overview of the directory structure as it is used is given below.  When downloading #TODO make sure to put the files in the nested folder `data/<NAMEDATASET>/`.  Most files should have a command-line argument for the data folder, but torchvision expects the data folder to contain the #TODO subdirectory.  


```tree
├── data
|    ├── <#TODO>
|    |   └── files
├── jobs
|    └── create_env.job
├── requirements.txt
└── main.py
```

## Environment and requirements

As the directory structure shows, we included a `requirements.txt` file with the `pip` packages that should be installed.  You can create a virtual environment with your favourite manager, i.e. conda, and install the requirements with:

```sh
# Create venv
conda create -n rl2 python=3.11 && conda activate rl2
# Install requirements
pip install -r requirements.txt
```

## Pipeline
#TODO

## Authors

This research project is a collaborative effort by Sergei Agaronian & Noa Nonkes, supervised by Roxana Petcu, from the University of Amsterdam.